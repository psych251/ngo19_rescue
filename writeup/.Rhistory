Independent_AcBc = Independent_Model_Ac_Bc,
Independent_AbCb = Independent_Model_Ab_Cb,
Dependency_AbAc = Dependency_AbAc,
Dependency_BaBc = Dependency_BaBc,
Dependency_CaCb = Dependency_CaCb,
Dependency_BaCa = Dependency_BaCa,
Dependency_AcBc = Dependency_AcBc,
Dependency_AbCb = Dependency_AbCb,
Data_model_overall = total_data_model,
Independent_model_overall = total_ind_model,
Dependency_overall = Dependency)
DT::datatable(summary_data)
if(!221 %in% all_data_cleaned$trial_index) {
print("Trial index 221 is missing before counting exclusions based on perfect accuracy.")
} else {
print("Trial index 221 is present before counting exclusions based on perfect accuracy.")
}
## count number of participants to exclude for having overall task accuracy == 100%
perfect_acc_exclusions_idx <- which(summary_data$Accuracy_mean == 1)
if(length(perfect_acc_exclusions_idx) > 0) {
summary_data_w_exclusions <- summary_data[perfect_acc_exclusions_idx,]
} else {
print("No exclusions based on accuracy made!")
}
num_acc_exclusions <- length(perfect_acc_exclusions_idx)
num_acc_exclusions
print("Final dataset check before exclusions:")
print(head(all_data_cleaned))
if(any(is.na(Dependency)) | any(is.nan(Dependency))) {
print("Dependency contains NA or NaN values, which are not valid for t-test.")
} else {
### testing key confirmatory analysis
key_test <- t.test(Dependency, alternative = "greater", mu = 0)
print(key_test)
### get bayes factor for key confirmatory analysis t-test
key_test_bf <- ttestBF(Dependency, mu = 0, nullInterval = c(0, Inf))
print(key_test_bf)
print(1/key_test_bf) ## BF in support of the null
}
if(any(is.na(Dependency)) | any(is.nan(Dependency))) {
print("Dependency contains NA or NaN values, which are not valid for t-test.")
} else {
### testing key confirmatory analysis
key_test <- t.test(Dependency, alternative = "greater", mu = 0)
print(key_test)
### get bayes factor for key confirmatory analysis t-test
key_test_bf <- ttestBF(Dependency, mu = 0, nullInterval = c(0, Inf))
print(key_test_bf)
print(1/key_test_bf) ## BF in support of the null
}
knitr::opts_chunk$set(echo = TRUE)
## Data exclusion / filtering
### clean data into long format
all_data_cleaned <- all_data |>
group_by(subject_id) |>
#filter(row_number() > 78 & row_number() <= 221) |>
#slice(78:221) |>
filter(trial_index >= 78 & trial_index <= 221) |>
ungroup() |>
filter(!is.na(correct))
if(!221 %in% all_data_cleaned$trial_index) {
print("Trial index 221 is missing")
} else {
print("Trial index 221 is present")
}
print("Data after filtering:")
print(head(all_data_cleaned))
# Group the cleaned data by subject_id, then count the number of rows for each group
participant_row_counts <- all_data_cleaned |>
group_by(subject_id) |>
summarise(num_rows = n())
# Print the row counts to see the number of rows for each participant
print(participant_row_counts)
all_data_cleaned <- all_data_cleaned |>
mutate(accuracy = ifelse(correct == "true", 1, 0)) |>
select(subject_id, trial_index, time_elapsed, rt, stimulus, task,
retrieval_group, response, correct_response, correct, accuracy) |>
separate(stimulus, c(NA, NA, NA, NA, "id"), sep = "_", remove = FALSE, extra = "merge", fill = "right")
if(!221 %in% all_data_cleaned$trial_index) {
print("Trial index 221 is missing")
} else {
print("Trial index 221 is present")
}
library(pwr)
## effect sizes
d_original <- 1.32
d_large <- .80
## power levels to test
powers <- c(.80, .90, .95)
## power analyses for t-tests with effect size reported in original paper
power_80_original <- pwr.t.test(d = d_original, power = powers[1], type = "one.sample", alternative = "greater")
power_90_original <- pwr.t.test(d = d_original, power = powers[2], type = "one.sample", alternative = "greater")
power_95_original <- pwr.t.test(d = d_original, power = powers[3], type = "one.sample", alternative = "greater")
## power analyses for t-tests with relatively large effect size (to be conservative)
power_80_large <- pwr.t.test(d = d_large, power = powers[1], type = "one.sample", alternative = "greater")
power_90_large <- pwr.t.test(d = d_large, power = powers[2], type = "one.sample", alternative = "greater")
power_95_large <- pwr.t.test(d = d_large, power = powers[3], type = "one.sample", alternative = "greater")
## function to automate printing of power analysis results
print_power_results <- function(results) {
print(paste(round(results$n), "participants will be needed to achieve", results$power, "power for an effect size of d =", results$d))
}
print_power_results(power_80_original)
print_power_results(power_90_original)
print_power_results(power_95_original)
print_power_results(power_80_large)
print_power_results(power_90_large)
print_power_results(power_95_large)
## Load Relevant Libraries and Functions
library(tidyverse)
library(BayesFactor)
## Import data
### get all data files from directory
data_files <- list.files(file.path("~/Desktop/ngo19_rescue/exp/data"), pattern = "*.csv", full.names = TRUE)
data_files
### read individual data files to list
all_data <- lapply(data_files, read.csv)
### unpack list as singular tidy data frame
all_data <- all_data |>
map_df(as_tibble)
if(!221 %in% all_data$trial_index) {
print("Trial index 221 is missing after import.")
} else {
print("Trial index 221 is present after import.")
}
### unpack participant demographics
demographics_trials <- all_data |>
subset(trial_type == "survey-html-form")
### string together json strings
json_text <- paste0(demographics_trials$response, collapse = ',')
json_text <- paste0('[', json_text, ']')
### unpack json as data frame
demographics_trials <- jsonlite::fromJSON(json_text) |>
as_tibble() |>
mutate(subject_id = rownames(demographics_trials), .before = "Gender")
### get participant demographic stats
#### age
mean_age <- mean(as.numeric(demographics_trials$Age))
sd_age <- sd(as.numeric(demographics_trials$Age))
min_age <- min(as.numeric(demographics_trials$Age))
max_age <- max(as.numeric(demographics_trials$Age))
#### gender
table(demographics_trials$Gender)
## Data exclusion / filtering
### clean data into long format
all_data_cleaned <- all_data |>
group_by(subject_id) |>
#filter(row_number() > 78 & row_number() <= 221) |>
#slice(78:221) |>
filter(trial_index >= 78 & trial_index <= 221) |>
ungroup() |>
filter(!is.na(correct))
if(!221 %in% all_data_cleaned$trial_index) {
print("Trial index 221 is missing")
} else {
print("Trial index 221 is present")
}
print("Data after filtering:")
print(head(all_data_cleaned))
# Group the cleaned data by subject_id, then count the number of rows for each group
participant_row_counts <- all_data_cleaned |>
group_by(subject_id) |>
summarise(num_rows = n())
# Print the row counts to see the number of rows for each participant
print(participant_row_counts)
all_data_cleaned <- all_data_cleaned |>
mutate(accuracy = ifelse(correct == "true", 1, 0)) |>
select(subject_id, trial_index, time_elapsed, rt, stimulus, task,
retrieval_group, response, correct_response, correct, accuracy) |>
separate(stimulus, c(NA, NA, NA, NA, "id"), sep = "_", remove = FALSE, extra = "merge", fill = "right")
if(!221 %in% all_data_cleaned$trial_index) {
print("Trial index 221 is missing")
} else {
print("Trial index 221 is present")
}
# Calculate accuracy summaries
accuracy_summary <- all_data_cleaned |>
filter(task == "response") |>
group_by(retrieval_group) |>
summarise(mean = mean(accuracy), sd = sd(accuracy), n = n(), sem = sd/sqrt(n))
print(accuracy_summary)
## Prepare data for analysis - create columns
### compute individual accuracy by grouping
accuracy_summary <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
group_by(retrieval_group) |>
summarise(mean = mean(accuracy), sd = sd(accuracy), n = n(), sem = sd/sqrt(n))
### compute accuracy by participant
accuracy_summary_part <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
group_by(subject_id) |>
summarise(mean = mean(accuracy), sd = sd(accuracy), n = n(), sem = sd/sqrt(n))
### compute overall accuracy
overall_accuracy_summary <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
summarise(mean = mean(accuracy), sd = sd(accuracy), n = n(), sem = sd/sqrt(n))
### compute Ab_Ac_Accuracy
Ab_Ac_Accuracy <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
filter(retrieval_group == "Ab" | retrieval_group == "Ac") |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
### compute Ba_Bc_Accuracy
Ba_Bc_Accuracy <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
filter(retrieval_group == "Ba" | retrieval_group == "Bc") |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
### compute Ca_Cb_Accuracy
Ca_Cb_Accuracy <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
filter(retrieval_group == "Ca" | retrieval_group == "Cb") |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
### compute Ba_Ca_Accuracy
Ba_Ca_Accuracy <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
filter(retrieval_group == "Ba" | retrieval_group == "Ca") |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
### compute Ac_Bc_Accuracy
Ac_Bc_Accuracy <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
filter(retrieval_group == "Ac" | retrieval_group == "Bc") |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
### compute Ab_Cb_Accuracy
Ab_Cb_Accuracy <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
filter(retrieval_group == "Ab" | retrieval_group == "Cb") |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
### compute 6 2x2 contingency tables for retrieval dependency key analysis
#### define dependent model function
compute_data_model <- function(group_one, group_two, num_events = 24) {
test_set <- all_data_cleaned |>
filter(task == "response") |> # only use retrieval data
select(subject_id, id, retrieval_group, accuracy) |> # select only the necessary columns
filter(retrieval_group == group_one | retrieval_group == group_two) |> # filter out relevant groups from function params
pivot_wider(id_cols = c(subject_id, id), names_from = retrieval_group, values_from = accuracy) |> # make long data frame wide for data dependent calculations
rowwise() |>
mutate(sum = sum(eval(parse(text = group_one)) + eval(parse(text = group_two)))) # get sums to compute all_correct and all_incorrect proportion of contingency table
# get unique participants
ps <- unique(all_data_cleaned$subject_id)
data_models <- rep(NA, length(ps))
for(ii in 1:length(ps)) {
# get data for just the one participant in the loop
data_subset <- test_set |>
filter(subject_id == ps[ii])
# get proportion of all correct
prop_all_correct <- sum(data_subset$sum == 2) / num_events
# get proportion of all incorrect
prop_all_incorrect <- sum(data_subset$sum == 0) / num_events
# compute data model and store per participant
data_model_calc <- prop_all_correct + prop_all_incorrect
data_models[ii] <- data_model_calc
}
return(data_models)
}
#### 1) compute Data_Ab_Ac
Data_Ab_Ac <- compute_data_model("Ab", "Ac")
#### 2) compute Data_Ba_Bc
Data_Ba_Bc <- compute_data_model("Ba", "Bc")
#### 3) compute Data_Ca_Cb
Data_Ca_Cb <- compute_data_model("Ca", "Cb")
#### 4) compute Data_Ba_Ca
Data_Ba_Ca <- compute_data_model("Ba", "Ca")
#### 5) compute Data_Ac_Bc
Data_Ac_Bc <- compute_data_model("Ac", "Bc")
#### 6) compute Data_Ab_Cb
Data_Ab_Cb <- compute_data_model("Ab", "Cb")
#### define independent model function
compute_ind_model <- function(group_one, group_two) {
P_AB <- all_data_cleaned |>
filter(retrieval_group == group_one) |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
P_AC <- all_data_cleaned |>
filter(retrieval_group == group_two) |>
group_by(subject_id) |>
summarise(mean = mean(accuracy))
cor_cor <- P_AB$mean * P_AC$mean
incor_cor <- P_AC$mean * (1 - P_AB$mean)
cor_incor <- P_AB$mean * (1 - P_AC$mean)
incor_incor <- (1 - P_AB$mean) * (1 - P_AC$mean)
return(cor_cor + incor_incor)
}
#### 1) compute Independent_Model_Ab_Ac
Independent_Model_Ab_Ac <- compute_ind_model("Ab", "Ac")
#### 2) compute Independent_Model_Ba_Bc
Independent_Model_Ba_Bc <- compute_ind_model("Ba", "Bc")
#### 3) compute Independent_Model_Ca_Cb
Independent_Model_Ca_Cb <- compute_ind_model("Ca", "Cb")
#### 4) compute Independent_Model_Ba_Ca
Independent_Model_Ba_Ca <- compute_ind_model("Ba", "Ca")
#### 5) compute Independent_Model_Ac_Bc
Independent_Model_Ac_Bc <- compute_ind_model("Ac", "Bc")
#### 6) compute Independent_Model_Ab_Cb
Independent_Model_Ab_Cb <- compute_ind_model("Ab", "Cb")
#### 1) compute Dependency_AbAc
Dependency_AbAc <- Data_Ab_Ac - Independent_Model_Ab_Ac
#### 2) compute Dependency_BaBc
Dependency_BaBc <- Data_Ba_Bc - Independent_Model_Ba_Bc
#### 3) compute Dependency_CaCb
Dependency_CaCb <- Data_Ca_Cb - Independent_Model_Ca_Cb
#### 4) compute Dependency_BaCa
Dependency_BaCa <- Data_Ba_Ca - Independent_Model_Ba_Ca
#### 5) compute Dependency_AcBc
Dependency_AcBc <- Data_Ac_Bc - Independent_Model_Ac_Bc
#### 6) compute Dependency_AbCb
Dependency_AbCb <- Data_Ab_Cb - Independent_Model_Ab_Cb
### compute Collapsed_Data
total_data_model <- (Data_Ab_Ac + Data_Ba_Bc + Data_Ca_Cb + Data_Ba_Ca + Data_Ac_Bc + Data_Ab_Cb) / 6
### compute Collapsed_Ind_Model
total_ind_model <- (Independent_Model_Ab_Ac + Independent_Model_Ba_Bc + Independent_Model_Ca_Cb + Independent_Model_Ba_Ca + Independent_Model_Ac_Bc + Independent_Model_Ab_Cb) / 6
### compute Dependency
Dependency <- total_data_model - total_ind_model
Dependency
Dependency_avg <- mean(Dependency)
Dependency_avg
## construct complete data frame with all relevant stats summarized for each participant
summary_data <- data.frame(subject_id = accuracy_summary_part$subject_id,
Accuracy_mean = accuracy_summary_part$mean,
Accuracy_sd = accuracy_summary_part$sd,
Accuracy_sem = accuracy_summary_part$sem,
Accuracy_n = accuracy_summary_part$n,
Accuracy_AbAc = Ab_Ac_Accuracy$mean,
Accuracy_BaBc = Ba_Bc_Accuracy$mean,
Accuracy_CaCb = Ca_Cb_Accuracy$mean,
Accuracy_BaCa = Ba_Ca_Accuracy$mean,
Accuracy_AcBc = Ac_Bc_Accuracy$mean,
Accuracy_AbCb = Ab_Cb_Accuracy$mean,
Data_AbAc = Data_Ab_Ac,
Data_BaBc = Data_Ba_Bc,
Data_CaCb = Data_Ca_Cb,
Data_BaCa = Data_Ba_Ca,
Data_AcBc = Data_Ac_Bc,
Data_AbCb = Data_Ab_Cb,
Independent_AbAc = Independent_Model_Ab_Ac,
Independent_BaBc = Independent_Model_Ba_Bc,
Independent_CaCb = Independent_Model_Ca_Cb,
Independent_BaCa = Independent_Model_Ba_Ca,
Independent_AcBc = Independent_Model_Ac_Bc,
Independent_AbCb = Independent_Model_Ab_Cb,
Dependency_AbAc = Dependency_AbAc,
Dependency_BaBc = Dependency_BaBc,
Dependency_CaCb = Dependency_CaCb,
Dependency_BaCa = Dependency_BaCa,
Dependency_AcBc = Dependency_AcBc,
Dependency_AbCb = Dependency_AbCb,
Data_model_overall = total_data_model,
Independent_model_overall = total_ind_model,
Dependency_overall = Dependency)
DT::datatable(summary_data)
## count number of participants to exclude for having overall task accuracy == 100%
perfect_acc_exclusions_idx <- which(summary_data$Accuracy_mean == 1)
if(length(perfect_acc_exclusions_idx) > 0) {
summary_data_w_exclusions <- summary_data[perfect_acc_exclusions_idx,]
} else {
print("No exclusions based on accuracy made!")
}
num_acc_exclusions <- length(perfect_acc_exclusions_idx)
num_acc_exclusions
print("Final dataset check before exclusions:")
print(head(all_data_cleaned))
if(any(is.na(Dependency)) | any(is.nan(Dependency))) {
print("Dependency contains NA or NaN values, which are not valid for t-test.")
} else {
### testing key confirmatory analysis
key_test <- t.test(Dependency, alternative = "greater", mu = 0)
print(key_test)
### get bayes factor for key confirmatory analysis t-test
key_test_bf <- ttestBF(Dependency, mu = 0, nullInterval = c(0, Inf))
print(key_test_bf)
print(1/key_test_bf) ## BF in support of the null
}
## compute 95% CI of acc mean assuming normal distribution
acc_mean <- mean(summary_data$Accuracy_mean)
acc_sd <- sd(summary_data$Accuracy_mean)
n <- 12
error <- qnorm(0.975)*acc_sd/sqrt(n)
lower_ci <- acc_mean - error
upper_ci <- acc_mean + error
knitr::include_graphics("figures/original_data.png")
### Overall Accuracy
#### make plot
acc_plot <- ggplot(summary_data, aes(x = "", y = Accuracy_mean)) +
geom_boxplot(width = 0.15, fill = "#ad93ad", outlier.shape = NA) +
geom_jitter(width = 0, col = "#5c475b", fill = "#FFFFFF", alpha = .7, shape = 21, size = 1, stroke = 1) +
xlab(" ") +
ylab("Overall Accuracy") +
scale_y_continuous(breaks = seq(0.25, 1.00, 0.25), limits = c(0.25, 1.00)) +
theme_classic() +
theme(panel.grid = element_blank(),
panel.border = element_blank(),
legend.title = element_blank(),
legend.text = element_text(color = "grey20", size = 12),
axis.text.x = element_text(color = "grey20", size = 16),
axis.text.y = element_text(color = "grey20", size = 16),
axis.title.x = element_text(color = "grey20", size = 16, face = "bold"),
axis.title.y = element_text(color = "grey20", size = 16, vjust = 0.5, face = "bold"))
### Proportion of Joint Retrieval
#### get relevant data in long format
summary_data_joint_ret_long <- pivot_longer(summary_data,
c(Data_model_overall, Independent_model_overall),
names_to = "source",
values_to = "prop_joint_retrieval") |>
rowwise() |>
mutate(source_cleaned = ifelse(source == "Data_model_overall", "Data", "Ind. Model")) |>
select(subject_id, source, source_cleaned, prop_joint_retrieval)
#### make plot
ret_plot <- ggplot(summary_data_joint_ret_long, aes(x = source_cleaned, y = prop_joint_retrieval, fill = source_cleaned)) +
geom_boxplot(width = 0.15, outlier.shape = NA, position = position_dodge(1)) +
geom_jitter(aes(col = source_cleaned, x = source_cleaned), position = position_jitter(width = 0), alpha = .7, shape = 21, size = 1, stroke = 1) +
xlab("Young Adults") +
ylab("Proportion of\nJoint Retrieval") +
scale_y_continuous(breaks = seq(0.40, 1.00, 0.1), limits = c(0.40, 1.00)) +
scale_fill_manual(values = c("#ad93ad", "#dbb5d6")) +
scale_color_manual(values = c("#5c475b", "#573350")) +
theme_classic() +
theme(panel.grid = element_blank(),
panel.border = element_blank(),
legend.title = element_blank(),
legend.text = element_text(color = "grey20", size = 10),
legend.position = "none",
axis.text.x = element_text(color = "grey20", size = 13),
axis.text.y = element_text(color = "grey20", size = 16),
axis.title.x = element_text(color = "grey20", size = 22, vjust = 0, face = "bold"),
axis.title.y = element_text(color = "grey20", size = 16, vjust = 0.5, face = "bold"))
### Dependency
#### make plot
dep_plot <- ggplot(summary_data, aes(x = "", y = Dependency_overall)) +
geom_boxplot(width = 0.15, fill = "#ad93ad", outlier.shape = NA) +
geom_jitter(width = 0, col = "#5c475b", fill = "#FFFFFF", alpha = .7, shape = 21, size = 1, stroke = 1) +
geom_hline(yintercept = 0.0, col = "red", lty = "dashed") +
xlab(" ") +
ylab("Dependency") +
scale_y_continuous(breaks = seq(-0.50, 0.50, 0.1), limits = c(-0.50, 0.50)) +
theme_classic() +
theme(panel.grid = element_blank(),
panel.border = element_blank(),
legend.title = element_blank(),
legend.text = element_text(color = "grey20", size = 12),
axis.text.x = element_text(color = "grey20", size = 16),
axis.text.y = element_text(color = "grey20", size = 16),
axis.title.x = element_text(color = "grey20", size = 16, face = "bold"),
axis.title.y = element_text(color = "grey20", size = 16, vjust = 0.5, face = "bold"))
### Overall Accuracy
#### make plot
acc_plot <- ggplot(summary_data, aes(x = "", y = Accuracy_mean)) +
geom_boxplot(width = 0.15, fill = "#ad93ad", outlier.shape = NA) +
geom_jitter(width = 0, col = "#5c475b", fill = "#FFFFFF", alpha = .7, shape = 21, size = 1, stroke = 1) +
xlab(" ") +
ylab("Overall Accuracy") +
scale_y_continuous(breaks = seq(0.25, 1.00, 0.25), limits = c(0.25, 1.00)) +
theme_classic() +
theme(panel.grid = element_blank(),
panel.border = element_blank(),
legend.title = element_blank(),
legend.text = element_text(color = "grey20", size = 12),
axis.text.x = element_text(color = "grey20", size = 16),
axis.text.y = element_text(color = "grey20", size = 16),
axis.title.x = element_text(color = "grey20", size = 16, face = "bold"),
axis.title.y = element_text(color = "grey20", size = 16, vjust = 0.5, face = "bold"))
### Proportion of Joint Retrieval
#### get relevant data in long format
summary_data_joint_ret_long <- pivot_longer(summary_data,
c(Data_model_overall, Independent_model_overall),
names_to = "source",
values_to = "prop_joint_retrieval") |>
rowwise() |>
mutate(source_cleaned = ifelse(source == "Data_model_overall", "Data", "Ind. Model")) |>
select(subject_id, source, source_cleaned, prop_joint_retrieval)
#### make plot
ret_plot <- ggplot(summary_data_joint_ret_long, aes(x = source_cleaned, y = prop_joint_retrieval, fill = source_cleaned)) +
geom_boxplot(width = 0.15, outlier.shape = NA, position = position_dodge(1)) +
geom_jitter(aes(col = source_cleaned, x = source_cleaned), position = position_jitter(width = 0), alpha = .7, shape = 21, size = 1, stroke = 1) +
xlab("Young Adults") +
ylab("Proportion of\nJoint Retrieval") +
scale_y_continuous(breaks = seq(0.40, 1.00, 0.1), limits = c(0.40, 1.00)) +
scale_fill_manual(values = c("#ad93ad", "#dbb5d6")) +
scale_color_manual(values = c("#5c475b", "#573350")) +
theme_classic() +
theme(panel.grid = element_blank(),
panel.border = element_blank(),
legend.title = element_blank(),
legend.text = element_text(color = "grey20", size = 10),
legend.position = "none",
axis.text.x = element_text(color = "grey20", size = 13),
axis.text.y = element_text(color = "grey20", size = 16),
axis.title.x = element_text(color = "grey20", size = 22, vjust = 0, face = "bold"),
axis.title.y = element_text(color = "grey20", size = 16, vjust = 0.5, face = "bold"))
### Dependency
#### make plot
dep_plot <- ggplot(summary_data, aes(x = "", y = Dependency_overall)) +
geom_boxplot(width = 0.15, fill = "#ad93ad", outlier.shape = NA) +
geom_jitter(width = 0, col = "#5c475b", fill = "#FFFFFF", alpha = .7, shape = 21, size = 1, stroke = 1) +
geom_hline(yintercept = 0.0, col = "red", lty = "dashed") +
xlab(" ") +
ylab("Dependency") +
scale_y_continuous(breaks = seq(-0.50, 0.50, 0.1), limits = c(-0.50, 0.50)) +
theme_classic() +
theme(panel.grid = element_blank(),
panel.border = element_blank(),
legend.title = element_blank(),
legend.text = element_text(color = "grey20", size = 12),
axis.text.x = element_text(color = "grey20", size = 16),
axis.text.y = element_text(color = "grey20", size = 16),
axis.title.x = element_text(color = "grey20", size = 16, face = "bold"),
axis.title.y = element_text(color = "grey20", size = 16, vjust = 0.5, face = "bold"))
print(acc_plot)
print(ret_plot)
print(dep_plot)
